{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJ-_7fjxIdts",
    "outputId": "ed93353b-b778-44cf-c50e-fc33c6c19886"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üîß Installation\n",
    "# Install the transformers-attention-viz package from PyPI\n",
    "\"\"\"\n",
    "!pip install transformers-attention-viz -q\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kli3veqmIwWO",
    "outputId": "eae11342-cb17-4d2f-be16-526d835e46ab"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üìö Import Required Libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Import visualization tools\n",
    "from attention_viz import AttentionVisualizer\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579,
     "referenced_widgets": [
      "257cf8fa58954d0c970e7c0f2d959348",
      "e41fe0a67c6a41a7bfe8d31f236a294d",
      "cab7c9ad5fab42d482fddf3874e710a4",
      "ca2b1ab2a3d146c1b89c414dadb119ab",
      "70f3b0a3878747d0ba25fecbe0ed3819",
      "9f480dd996a84c25b417b8a774c0a712",
      "bbf3746dda4c4d349704a24b368d2c25",
      "cd7ca33df8f641a0a6b1869ac040a72c",
      "b40b7ad6c3d1492fa982d989ce020fa8",
      "202ba68499654686ad0264676bcd834f",
      "3b6302a325354aa2a78cbba7fd95ac65",
      "86c3c18d1a2b44f8bea931deb204a12a",
      "30fc9babcf1042ddb9c58d6c9d7d5b13",
      "e2fef88842ee484cbb6989b85fc363cf",
      "777e47b1f2fd46fdb4e11685f062758b",
      "49be7b939f6244379a222c43b53ed94d",
      "38739df0cd5d4cf78d6eea5e3161aa06",
      "4a24e09f425a477d9ebf5ad174eb55d2",
      "bdf220e48a144af8a90babade8ebd5b9",
      "241b89b514bf456c80fc4619c70c280a",
      "bb9b55dc1ff3476cae5ceac204b20ef9",
      "f89a184373a04ed5b1a69cf89d87dc71",
      "bbacaf382f1442749115af89e78293d1",
      "93814a97199c4e78a9139bbd048e4e24",
      "6f9ab3e37cea43c0973bd10c680a918f",
      "563c178fe1194963916f7f57e6130566",
      "3c5fb785464f497d9d27449ca1888bf4",
      "a3b91010c9af4cfbbf053e9b3d656127",
      "7f6a1c2feda543dc9cb5153c9f86d665",
      "a1098555e8a74189803b46321adf2fb6",
      "80bee0a3ab7e4b8f80a574a3c5eb3a36",
      "bc982472a57a4045a121df08db4d3392",
      "9006c533a9994118b92881dc5c078b0f",
      "ee0f4f8f964a40cb8ac0b884a48d982f",
      "56bca3de75b24c54830da11b209a6ea1",
      "f71466cdb8f34cf69fadfe6079ab6f01",
      "dd5f46cd1cd544269073d2febe9e019c",
      "6813fb40d2c241c48ecfee2025627be1",
      "b3188c9e75534f5284368f8b71239691",
      "189b57198a784ebfaf0371c7686b21e8",
      "a7dffad88761469381f9b65ec36de3b4",
      "035ff172449243d5b9a2ee3d729ca90d",
      "7ad64f25fd514f95be8e4237829a3a16",
      "412eae49d902423fa1781b614dabe999",
      "d8e22ef12a12400eb98eedecf28a1648",
      "457dad980809438a89a3154cae988b05",
      "5bc818fef0a448f5a6a1422093b333c0",
      "91bf40eac8ac4b438b125b5511a43aaf",
      "9596ed8ba72748839703f7167a278633",
      "ea1fd52110234180a11eb9ad664fdd16",
      "57c1a3c0ba5940dc9d69674c8bb2469c",
      "62de50c579e44c7f91c19d1fc9467775",
      "e6e8e82bc07c4e3cb27eaa085c10126e",
      "fd32323e85f74e9aaff0c2e02d813a68",
      "15d3b89ad2a4488ca5e19a97e1594a0e",
      "3502967efe584583a8b2936df18fd43e",
      "b96635804141426c8dfe548effa1680d",
      "3d0e402495b2433a98988f5de74b64ca",
      "848816628ff8480ba08702671a88ad5d",
      "1ee4331cf1bf42fca108052feeec804a",
      "1f744a835a6b4ce88199bc60cd73a120",
      "f2655c24625c41ac9233704643e8f19c",
      "777fa85f0fd44e1cb6fd2edd53d1a400",
      "85766bbf7230458f89b11ac676ca23da",
      "f61a59c31cd9461689596b5b44623fce",
      "f39b49f93f0c44cc9da165e19ec46f79",
      "7318bb97a47344a0afb16e84e8572098",
      "99551e8257e044f68e02f6adb4dca8a8",
      "bfcde3fdc26e49528383048bf1d6f840",
      "f920267a1b614da193c3afb80c2dbfd0",
      "383ae4a54982421d9ad2ff6fa6406551",
      "8e45c4bbc1294fd3a4b61d31bc352edf",
      "9d054581967649a190fd68df78bcecb9",
      "aa55a3fc971a47f694f1e64885c0e544",
      "ab72c09d89af46da98f7d6be13af7b8c",
      "006e8d20fcf34a1eb3cccbbcdf034bcb",
      "0d23eab3db634cf58787cec0e4a32066",
      "4675f7b142ca4aceb87eb0f1944c6312",
      "ca4f2b0669a347398b13244fd2eac8e7",
      "7713027d76dd4a469e6ac5dbaa750308",
      "63a9b5153a2d4b0aabdcb03cab05f917",
      "8b59b112a4c849569c682428e1e276ae",
      "68ecbc432fb744188356fd23086adf92",
      "d0c83162cfc64912876ae08fb79415b7",
      "4cd3c74e2f1b486cb20023327e4cd75b",
      "c782441701a540e1b71fe226b3ecb86f",
      "9b829ea1321443b6b91b1f630c273e42",
      "160f710fc0cd4805b631fb832e804868"
     ]
    },
    "id": "kpUEKU5yJPZl",
    "outputId": "ce19b1de-c94d-47ff-86eb-abca3e158bb7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ü§ñ Load Pre-trained Models\n",
    "# We'll use BLIP for cross-modal attention visualization\n",
    "\"\"\"\n",
    "print(\"Loading BLIP model (this may take a minute)...\")\n",
    "\n",
    "# Load BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = AttentionVisualizer(blip_model, blip_processor)\n",
    "\n",
    "print(\"‚úÖ Model loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "7k6abVbhJcft",
    "outputId": "2417d425-3f95-456f-8a21-ac80e5fc81a2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üñºÔ∏è Load Example Image\n",
    "# Let's use a cat image for demonstration\n",
    "\"\"\"\n",
    "# Load image from URL\n",
    "image_url = \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\"\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Example Image: Cat\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZaTEqfynJhxL",
    "outputId": "b21c2dac-aa6b-4614-e4f8-f5597aab6a0c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üîç Visualize Cross-Modal Attention\n",
    "# This shows how text tokens attend to image regions\n",
    "\"\"\"\n",
    "# Define the text description\n",
    "text = \"a fluffy orange cat sitting on a surface\"\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(\"\\nGenerating cross-modal attention visualization...\")\n",
    "\n",
    "# Create visualization\n",
    "viz = visualizer.visualize(\n",
    "    image=image,\n",
    "    text=text,\n",
    "    visualization_type=\"heatmap\",\n",
    "    attention_type=\"cross\"\n",
    ")\n",
    "\n",
    "# Display the visualization\n",
    "viz.show()\n",
    "\n",
    "print(\"\\nüí° Each subplot shows how one text token attends to the image patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTopbXPwJqnj",
    "outputId": "beef4806-9829-4313-9171-b9267ab33d8d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üìä Analyze Attention Patterns\n",
    "# Get quantitative metrics about the attention distribution\n",
    "\"\"\"\n",
    "# Calculate attention statistics\n",
    "stats = visualizer.get_attention_stats(image, text, attention_type=\"cross\")\n",
    "\n",
    "print(\"üìà Attention Statistics:\")\n",
    "print(f\"‚Ä¢ Model type: {stats['model_type']}\")\n",
    "print(f\"‚Ä¢ Average entropy: {np.mean(stats['entropy']):.3f}\")\n",
    "print(f\"‚Ä¢ Attention concentration: {stats['concentration']:.3f}\")\n",
    "print(f\"\\nüéØ Top 5 most attended image regions:\")\n",
    "for i, (patch, score) in enumerate(stats['top_tokens'][:5]):\n",
    "    print(f\"  {i+1}. {patch}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tCjiY6fSKKKX",
    "outputId": "4b1f3eda-09cf-4033-e72d-1c1a1868245f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üîÑ Compare Different Descriptions\n",
    "# See how attention changes with different text\n",
    "\"\"\"\n",
    "# Test different descriptions\n",
    "descriptions = [\n",
    "    \"cat\",\n",
    "    \"orange cat\",\n",
    "    \"fluffy cat sitting\"\n",
    "]\n",
    "\n",
    "print(\"Comparing attention patterns for different descriptions:\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, desc in enumerate(descriptions):\n",
    "    # Generate visualization\n",
    "    viz = visualizer.visualize(\n",
    "        image=image,\n",
    "        text=desc,\n",
    "        visualization_type=\"heatmap\",\n",
    "        attention_type=\"cross\"\n",
    "    )\n",
    "\n",
    "    # For display, we'll show just the first content token's attention\n",
    "    # (skipping CLS token)\n",
    "    stats = visualizer.get_attention_stats(image, desc, attention_type=\"cross\")\n",
    "\n",
    "    # Create a simple heatmap for comparison\n",
    "    axes[idx].set_title(f'\"{desc}\"\\n({len(desc.split())+2} tokens)')\n",
    "    axes[idx].text(0.5, 0.5, f\"Entropy: {np.mean(stats['entropy']):.3f}\",\n",
    "                   ha='center', va='center', transform=axes[idx].transAxes)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Longer descriptions create more tokens and different attention patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yv1anu6jKSfO",
    "outputId": "853174ac-f0c5-4c26-89fa-7977d7e4c247"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üèóÔ∏è Analyze Different Transformer Layers\n",
    "# Attention patterns evolve through the network layers\n",
    "\"\"\"\n",
    "print(\"Visualizing attention at different layers...\\n\")\n",
    "\n",
    "# Visualize first, middle, and last layers\n",
    "layer_indices = [0, 5, 11]\n",
    "layer_names = [\"First Layer\", \"Middle Layer\", \"Last Layer\"]\n",
    "\n",
    "for layer_idx, layer_name in zip(layer_indices, layer_names):\n",
    "    print(f\"üìç {layer_name} (Layer {layer_idx}):\")\n",
    "\n",
    "    viz = visualizer.visualize(\n",
    "        image=image,\n",
    "        text=\"cat\",\n",
    "        visualization_type=\"heatmap\",\n",
    "        attention_type=\"cross\",\n",
    "        layer_indices=[layer_idx]\n",
    "    )\n",
    "\n",
    "    # Get stats for this specific layer\n",
    "    stats = visualizer.get_attention_stats(\n",
    "        image, \"cat\",\n",
    "        attention_type=\"cross\",\n",
    "        layer_index=layer_idx\n",
    "    )\n",
    "\n",
    "    print(f\"   Entropy: {np.mean(stats['entropy']):.3f}\")\n",
    "    print(f\"   Top patch: {stats['top_tokens'][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0_3ey82YKfrb",
    "outputId": "8bb9ee49-c76f-4d38-e9c8-21b152a95cbf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üìà Attention Evolution Across Layers\n",
    "# See how attention patterns change through the network\n",
    "\"\"\"\n",
    "print(\"Generating attention evolution visualization...\\n\")\n",
    "\n",
    "try:\n",
    "    evolution_viz = visualizer.visualize(\n",
    "        image=image,\n",
    "        text=\"cat\",\n",
    "        visualization_type=\"evolution\",\n",
    "        attention_type=\"cross\"\n",
    "    )\n",
    "    evolution_viz.show()\n",
    "    print(\"‚úÖ Evolution visualization shows how attention changes across layers\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Evolution visualization may have minor issues with some configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "zEILTs9hKu2b",
    "outputId": "8086cd03-8164-42ec-e238-6fc77c5b2866"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üíæ Export Visualizations\n",
    "# Save attention visualizations for papers or presentations\n",
    "\"\"\"\n",
    "# Create a visualization to export\n",
    "export_viz = visualizer.visualize(\n",
    "    image=image,\n",
    "    text=\"fluffy cat\",\n",
    "    visualization_type=\"heatmap\",\n",
    "    attention_type=\"cross\"\n",
    ")\n",
    "\n",
    "# Export in different formats\n",
    "print(\"Exporting visualizations...\\n\")\n",
    "\n",
    "# PNG format (for papers)\n",
    "export_viz.save(\"attention_visualization.png\", dpi=300)\n",
    "print(\"‚úÖ Saved as PNG (300 DPI) - perfect for papers\")\n",
    "\n",
    "# PDF format (for LaTeX)\n",
    "export_viz.save(\"attention_visualization.pdf\")\n",
    "print(\"‚úÖ Saved as PDF - great for LaTeX documents\")\n",
    "\n",
    "# SVG format (for web)\n",
    "export_viz.save(\"attention_visualization.svg\")\n",
    "print(\"‚úÖ Saved as SVG - scalable for web use\")\n",
    "\n",
    "# Show file sizes\n",
    "import os\n",
    "for ext in ['png', 'pdf', 'svg']:\n",
    "    filename = f\"attention_visualization.{ext}\"\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"   {filename}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ktVeFprQLAru",
    "outputId": "3a05573c-3a7c-49bd-faeb-da5e498032de"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üé® Try Different Types of Images\n",
    "# The toolkit works with any image type\n",
    "\"\"\"\n",
    "# Test with different image subjects\n",
    "test_images = {\n",
    "    \"Dog\": \"https://images.unsplash.com/photo-1543466835-00a7907e9de1?w=400\",\n",
    "    \"Landscape\": \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\",\n",
    "    \"Food\": \"https://images.unsplash.com/photo-1565299624946-b28f40a0ae38?w=400\"\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (subject, url) in enumerate(test_images.items()):\n",
    "    # Load image\n",
    "    response = requests.get(url)\n",
    "    test_image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Display original\n",
    "    axes[0, idx].imshow(test_image)\n",
    "    axes[0, idx].set_title(f\"{subject} Image\")\n",
    "    axes[0, idx].axis('off')\n",
    "\n",
    "    # Simple description\n",
    "    descriptions = {\n",
    "        \"Dog\": \"happy dog\",\n",
    "        \"Landscape\": \"mountain view\",\n",
    "        \"Food\": \"delicious food\"\n",
    "    }\n",
    "\n",
    "    # Generate visualization\n",
    "    viz = visualizer.visualize(\n",
    "        image=test_image,\n",
    "        text=descriptions[subject],\n",
    "        visualization_type=\"heatmap\",\n",
    "        attention_type=\"cross\"\n",
    "    )\n",
    "\n",
    "    # For display purposes, show a placeholder\n",
    "    axes[1, idx].text(0.5, 0.5, f\"'{descriptions[subject]}'\\nattention heatmap\",\n",
    "                      ha='center', va='center', fontsize=12)\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ The toolkit works with any type of image!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1534abc1b8f34bc5bc452b25076dabd3",
      "1d1ea4b93e204c0383707381d969894d",
      "8c3f1165e6004091a3c398ca8ac49735",
      "a976e2466b0e4353a17ff197273d4b05",
      "df88b220793c473aab6281a36b90f632",
      "3dca631898b94cdabedb634177a25748",
      "e0cba54edd7848579b4b674625456785",
      "42aafc0a09314999a2180fdde4b69ec4",
      "9ac24ad624fc44089f5200e4ed5cb2fa",
      "94f66745caa44648ac697b618602c842",
      "d91236b84cb54292934918e9ff781e5a",
      "1b33d0b8d1b74d7c937e5cae74e7075c",
      "9724cc6366a746cfa6ac5a3c9b746f6d",
      "2a744c330cea4029b5b33bf9eb9195c3",
      "071a7497bf06455f9c1d8eb33b41c748",
      "6ec82ec07c19467eaf1e6f633f5adea2",
      "81428bf549ed4dc09eca25e34e04f8e7",
      "4c110d71b0484f0ba33b09629b0e4d6e",
      "f375f05f96de431e9595f14a7fc8f3c1",
      "97daefc6cf9c4bd3bba19ea5ea681510",
      "3da44677fdf74a5688960344abbaae76",
      "bda35e25b79542159d9d2e1e692a96f4",
      "2c22d3ffc9124c23bc320a7c94a30042",
      "1159030c6cfc402787d2c426a995e66e",
      "1b4d8b279ea34dea93ffbe09ed29e772",
      "b410f27bc1554951913969fb58d1bd65",
      "fd5dbefd81bb4f67a8c7e076ba451009",
      "3f03d0f8afd3436aaa27107b517b5c53",
      "3381fc865ce34ea3b07c2eae8f61e188",
      "9cc1f1c3910049c596b98df39d9fc710",
      "ac3f738e8a3845cd82138227f63293c5",
      "3341c7f3916d42aa91a7ad4f69213af6",
      "9cdea1b557c0488b86b133e31d5180d7",
      "6f04e70ea50b4c1c838dcef82d3dc223",
      "02f98eedbc9d428b84c88b9605544918",
      "e2df161d0b204b8982b5a6d8dc484993",
      "90b9bf199d634a1ba812e47a008ef259",
      "d794138e6f384140959774922c06f383",
      "0923d65d80f74f1abe0a82604559f574",
      "c2e4b1c8d81c440c806fb7b7e96b3f40",
      "150d82191e6d4090aa4d1246658a3350",
      "0aa117adc2404d9ba3512dda328e1c4e",
      "9895e8bb75864fa4959d16921f5eaccf",
      "0da8b4eb7aba425aa5351f0a0db5759c",
      "11963b4850ad4e309ed2c0af2145da44",
      "0d68d5c5cec249729e893d6b30b2b356",
      "750a31dee19e42509610266367f27722",
      "68821f5ec3114a39954e17f1b8b93d61",
      "3276812431b346a3a7299697d03e7f6d",
      "cdc3bb3b9a784a5a857a58212d6fb709",
      "bdb6c6615edd410fa76a6570fa0d1afc",
      "e0a5a9dae3654baa8f2572cd7dd633ce",
      "da42e2a128f64a85924675cc5493d0a9",
      "f1f3d7dad9264414a61c2bed6dc1cce1",
      "194ca07bf5e941ea89f603e0344a9e61",
      "66fc7a6d91b4445d8f6cc74a9e03c627",
      "bf93b2e3c5454a87bc9ed3a710b61d75",
      "2b602fe9c5c14790a6cb7dcbf61f0c4a",
      "605d928ba83d4cb8b540b576cc39df75",
      "ff37a3ca7c434476bc0786ec83afffa1",
      "f14acb2944734ec48a2b721216409b3a",
      "29ea412e630b4b8fae19b8578818d0aa",
      "13034a72933a4b21a8b8e8418cf616ac",
      "4ea041cb56d04e188a3123f9bf0366ba",
      "67217a7b41594b60a19174d7b34283ec",
      "fe716815b6ec45bfa2c4bedbcce75eab",
      "f3c7211270724e419e0e6b941754989e",
      "5c387622b66d4c088ad657336b5197f3",
      "fcd56bacc454449294ec0aea68fd14f6",
      "1925882789d641bcadb3dff1ff5dc836",
      "e287614f76d244e2ac222f95a04ad993",
      "cc9b942e22dc4d23a5a300aa299d0a07",
      "b06a44d7d40e48698678ce8809d0dd69",
      "a9a20792d670463b9ebca55035ac6455",
      "0ede133733ab43b4a4efd74d97548fe0",
      "4370f5e20da142058ba89aac14490c35",
      "ed6f35446c204088bd179d0931af117d",
      "9f7300ac74284decbc85a4ee76f45f66",
      "4c55996358454453aa98dda1d58b80dc",
      "62a6b6ef3ab1487b99f0c0b08b3cc06a",
      "e64da8c92f6740739d2375e6b7ea9317",
      "79ed074c15ac4db6974daca10a219cac",
      "555a34bb64314bfa9b5e5de0122e4896",
      "e52cffd8505c4889ab753e239e1757b5",
      "a4ec4f57ba464f58a814444c8871c8a1",
      "921e096eeefe44d9b50154e81af4b37e",
      "e85a792f6a464e3aa247ad64634273a9",
      "7c0c25eaead8426fa736c4b52d651896",
      "b458ec560c0d4c7895eb5173fe7d8d3d",
      "94c11e7ab30343fc85ffcc7fa7dfb6a5",
      "11aef565eb1d4393b51cd9db8b093bcc",
      "82a54a1346ef4c04bf6da1f9c3c40f31",
      "8e8d793368664b72a231c62daaba1686",
      "b59b884e3d484bb394d7bef72115ef4f",
      "96b9371bb1f04c4a9b7501cefd202241",
      "ec24983e35f54e18801e76f0f3f2f4d4",
      "a6727b32de4245aa827a5156d6eb8ba0",
      "b759da12bcf14ea987ad4b51b849dbe3",
      "698c021a416e423ebf578940890a67a5"
     ]
    },
    "id": "2IPs-BaTLKHA",
    "outputId": "b72b4466-b554-4872-dca9-5ad033d30714"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üîÄ CLIP Model Compatibility\n",
    "# The toolkit also supports CLIP (vision self-attention only)\n",
    "\"\"\"\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_visualizer = AttentionVisualizer(clip_model, clip_processor)\n",
    "\n",
    "print(\"‚úÖ CLIP model loaded\")\n",
    "print(\"\\nNote: CLIP doesn't have cross-modal attention.\")\n",
    "print(\"Visualizing vision self-attention instead...\\n\")\n",
    "\n",
    "# Visualize CLIP's vision self-attention\n",
    "clip_viz = clip_visualizer.visualize(\n",
    "    image=image,\n",
    "    text=\"cat\",\n",
    "    visualization_type=\"heatmap\",\n",
    "    attention_type=\"vision_self\"\n",
    ")\n",
    "\n",
    "clip_viz.show()\n",
    "\n",
    "print(\"\\nüí° This shows how image patches attend to each other in CLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MohLm4baLZ3W",
    "outputId": "45c8abc8-a566-405d-b917-46b06145c3f5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üéØ Summary and Next Steps\n",
    "\"\"\"\n",
    "print(\"üéâ Congratulations! You've learned how to use transformers-attention-viz\\n\")\n",
    "\n",
    "print(\"üìö What you've learned:\")\n",
    "print(\"‚Ä¢ Visualize cross-modal attention in BLIP\")\n",
    "print(\"‚Ä¢ Analyze attention statistics and metrics\")\n",
    "print(\"‚Ä¢ Compare attention patterns across descriptions\")\n",
    "print(\"‚Ä¢ Export visualizations for publications\")\n",
    "print(\"‚Ä¢ Work with different model types (BLIP, CLIP)\")\n",
    "\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"‚Ä¢ Try with your own images and descriptions\")\n",
    "print(\"‚Ä¢ Explore attention patterns in your research\")\n",
    "print(\"‚Ä¢ Use exported visualizations in papers\")\n",
    "print(\"‚Ä¢ Contribute to the project on GitHub\")\n",
    "\n",
    "print(\"\\nüìñ Resources:\")\n",
    "print(\"‚Ä¢ GitHub: https://github.com/sisird864/transformers-attention-viz\")\n",
    "print(\"‚Ä¢ PyPI: https://pypi.org/project/transformers-attention-viz/\")\n",
    "print(\"‚Ä¢ Documentation: See GitHub README\")\n",
    "\n",
    "print(\"\\n‚≠ê If you find this useful, please star the GitHub repo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "__J4RnOvLk2s",
    "outputId": "5b01f0ce-9294-48c9-c55a-0268ffa782d2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# üñ•Ô∏è Interactive Dashboard (Optional)\n",
    "# For local use only - not available in Colab\n",
    "\"\"\"\n",
    "print(\"üí° Advanced Feature: Interactive Dashboard\\n\")\n",
    "\n",
    "print(\"The package includes a Gradio-based interactive dashboard.\")\n",
    "print(\"To use it locally:\\n\")\n",
    "\n",
    "print(\"```python\")\n",
    "print(\"from transformers_attention_viz import launch_dashboard\")\n",
    "print(\"launch_dashboard(model, processor)\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nThis provides:\")\n",
    "print(\"‚Ä¢ Real-time attention exploration\")\n",
    "print(\"‚Ä¢ Model switching interface\")\n",
    "print(\"‚Ä¢ Parameter adjustment controls\")\n",
    "print(\"‚Ä¢ Direct export functionality\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: The dashboard requires a local environment (doesn't work in Colab)\")\n",
    "\n",
    "# End of notebook\n",
    "\"\"\"\n",
    "# üôè Thank You!\n",
    "# We hope you find this tool useful for your research!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
