{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b1766",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f9e86c6",
   "metadata": {},
   "source": [
    "# Transformers Attention Visualization - Basic Usage\n",
    "\n",
    "This notebook demonstrates how to use the transformers-attention-viz library to visualize attention patterns in multi-modal transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5940c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install the library (if not already installed)\n",
    "# !pip install transformers-attention-viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684a490",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from attention_viz import AttentionVisualizer\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff73fff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 1. Load Model and Create Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f42d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = AttentionVisualizer(model, processor)\n",
    "print(f\"Loaded {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a334c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 2. Load Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7462b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load an example image from URL\n",
    "url = \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "image.resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ec229",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 3. Basic Attention Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49166b5f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create attention heatmap\n",
    "text = \"a photo of a cat\"\n",
    "\n",
    "viz = visualizer.visualize(\n",
    "    image=image,\n",
    "    text=text,\n",
    "    visualization_type=\"heatmap\",\n",
    "    layer_indices=-1  # Last layer\n",
    ")\n",
    "\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806746c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 4. Attention Flow Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdd541",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create attention flow visualization\n",
    "viz_flow = visualizer.visualize(\n",
    "    image=image,\n",
    "    text=text,\n",
    "    visualization_type=\"flow\",\n",
    "    threshold=0.1,  # Only show connections above this threshold\n",
    "    top_k=5  # Show top 5 connections per token\n",
    ")\n",
    "\n",
    "viz_flow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afa4a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 5. Layer Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2290701",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze how attention evolves across layers\n",
    "viz_evolution = visualizer.visualize(\n",
    "    image=image,\n",
    "    text=text,\n",
    "    visualization_type=\"evolution\",\n",
    "    metric=\"entropy\"  # Track entropy across layers\n",
    ")\n",
    "\n",
    "viz_evolution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a03fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 6. Attention Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbf339",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get detailed attention statistics\n",
    "stats = visualizer.get_attention_stats(image, text)\n",
    "\n",
    "print(\"Attention Statistics:\")\n",
    "print(f\"- Average Entropy: {stats['entropy'].mean():.3f}\")\n",
    "print(f\"- Attention Concentration (Gini): {stats['concentration']:.3f}\")\n",
    "print(\"\\nTop Attended Tokens:\")\n",
    "for token, score in stats['top_tokens']:\n",
    "    print(f\"  - {token}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716eb70e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 7. Compare Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c74c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compare attention patterns for different text descriptions\n",
    "texts = [\"a cat\", \"a fluffy orange cat\", \"a cat sitting on a couch\"]\n",
    "\n",
    "comparison = visualizer.compare_attention(\n",
    "    images=[image] * 3,  # Same image, different texts\n",
    "    texts=texts,\n",
    "    layer_index=-1\n",
    ")\n",
    "\n",
    "comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba0f60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 8. Save Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a107e64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save visualization to file\n",
    "viz.save(\"attention_heatmap.png\", dpi=300)\n",
    "print(\"Saved attention_heatmap.png\")\n",
    "\n",
    "# Convert to PIL Image for further processing\n",
    "pil_image = viz.to_image()\n",
    "print(f\"Image size: {pil_image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1f72e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 9. Launch Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a74aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Launch the interactive dashboard\n",
    "from attention_viz import launch_dashboard\n",
    "\n",
    "# Uncomment to launch (will open in browser)\n",
    "# launch_dashboard(model, processor)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
