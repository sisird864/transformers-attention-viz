{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e949bddd",
   "metadata": {},
   "source": [
    "# üîç Transformers Attention Viz Demo\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sisird864/transformers-attention-viz/blob/main/examples/demo.ipynb)\n",
    "[![PyPI](https://img.shields.io/pypi/v/transformers-attention-viz.svg)](https://pypi.org/project/transformers-attention-viz/)\n",
    "\n",
    "This notebook demonstrates how to visualize attention patterns in multi-modal transformers like CLIP and BLIP.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to install and use transformers-attention-viz\n",
    "- Visualizing cross-modal attention between text and images\n",
    "- Analyzing attention statistics\n",
    "- Using the interactive dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e727c6",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "First, let's install the package and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f2a1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers-attention-viz torch transformers pillow requests -q\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67fae",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "\n",
    "Let's visualize how CLIP understands the relationship between images and text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5726dba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from attention_viz import AttentionVisualizer\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = AttentionVisualizer(model, processor)\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029971ab",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Example 1: Understanding a Cat Image\n",
    "\n",
    "Let's see how CLIP processes an image of a cat with descriptive text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a4532",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load a cat image\n",
    "url = \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image\")\n",
    "plt.show()\n",
    "\n",
    "# Create attention visualization\n",
    "text = \"a fluffy orange cat sitting on a surface\"\n",
    "print(f\"Text: '{text}'\")\n",
    "\n",
    "viz = visualizer.visualize(\n",
    "    image=image,\n",
    "    text=text,\n",
    "    visualization_type=\"heatmap\"\n",
    ")\n",
    "\n",
    "# Display the attention heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(viz.to_image())\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65986f",
   "metadata": {},
   "source": [
    "## üìä Attention Statistics\n",
    "\n",
    "Let's analyze the attention patterns to see which tokens the model focuses on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c4993",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get attention statistics\n",
    "stats = visualizer.get_attention_stats(image, text)\n",
    "\n",
    "print(\"üîç Attention Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Average Entropy: {stats['entropy'].mean():.3f}\")\n",
    "print(f\"Attention Concentration: {stats['concentration']:.3f}\")\n",
    "print(\"\\nüèÜ Top 5 Most Attended Tokens:\")\n",
    "for i, (token, score) in enumerate(stats['top_tokens'][:5]):\n",
    "    print(f\"{i+1}. '{token}' - {score*100:.1f}% attention\")\n",
    "\n",
    "# Visualize attention distribution\n",
    "tokens = [t[0] for t in stats['top_tokens'][:10]]\n",
    "scores = [t[1] for t in stats['top_tokens'][:10]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(tokens)), scores)\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.title('Top 10 Tokens by Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400b25d",
   "metadata": {},
   "source": [
    "## üéØ Example 2: Comparing Different Descriptions\n",
    "\n",
    "Let's see how attention changes with different text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35607aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "descriptions = [\n",
    "    \"a cat\",\n",
    "    \"an orange cat\",\n",
    "    \"a fluffy cat sitting\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, desc in enumerate(descriptions):\n",
    "    viz = visualizer.visualize(image, desc, visualization_type=\"heatmap\")\n",
    "    axes[idx].imshow(viz.to_image())\n",
    "    axes[idx].set_title(f\"Text: '{desc}'\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Attention Patterns for Different Descriptions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab6841",
   "metadata": {},
   "source": [
    "## üåü Example 3: Different Image Types\n",
    "\n",
    "Let's test with different types of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400933a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with different images\n",
    "test_cases = [\n",
    "    (\"https://images.unsplash.com/photo-1547407139-3c921a66005c?w=400\", \"a golden retriever dog\"),\n",
    "    (\"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\", \"snowy mountain peaks\"),\n",
    "    (\"https://images.unsplash.com/photo-1449824913935-59a10b8d2000?w=400\", \"city street with buildings\")\n",
    "]\n",
    "\n",
    "for img_url, description in test_cases:\n",
    "    # Load image\n",
    "    response = requests.get(img_url)\n",
    "    test_image = Image.open(BytesIO(response.content))\n",
    "    \n",
    "    # Create visualization\n",
    "    viz = visualizer.visualize(test_image, description, visualization_type=\"heatmap\")\n",
    "    \n",
    "    # Get stats\n",
    "    stats = visualizer.get_attention_stats(test_image, description)\n",
    "    \n",
    "    # Display\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.imshow(test_image)\n",
    "    ax1.set_title(f\"Input: '{description}'\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(viz.to_image())\n",
    "    top_token = stats['top_tokens'][0]\n",
    "    ax2.set_title(f\"Top token: '{top_token[0]}' ({top_token[1]*100:.1f}%)\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078af214",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Dashboard\n",
    "\n",
    "For a more interactive experience, you can launch the Gradio dashboard locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f494c0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Note: This will not work in Colab, but you can run it locally!\n",
    "print(\"To launch the interactive dashboard locally, run:\")\n",
    "print(\"\\nfrom attention_viz import launch_dashboard\")\n",
    "print(\"launch_dashboard(model, processor)\")\n",
    "print(\"\\nOr from command line:\")\n",
    "print(\"attention-viz-dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f189cb",
   "metadata": {},
   "source": [
    "## üîß Advanced Usage\n",
    "\n",
    "### Analyzing Specific Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9b4cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize attention from different layers\n",
    "layers_to_check = [0, 5, -1]  # First, middle, and last layer\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, layer in enumerate(layers_to_check):\n",
    "    viz = visualizer.visualize(\n",
    "        image=image,\n",
    "        text=\"a cat\",\n",
    "        layer_indices=[layer],\n",
    "        visualization_type=\"heatmap\"\n",
    "    )\n",
    "    axes[idx].imshow(viz.to_image())\n",
    "    layer_name = \"Last\" if layer == -1 else f\"Layer {layer}\"\n",
    "    axes[idx].set_title(f\"{layer_name}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Attention at Different Layers\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70eae7a",
   "metadata": {},
   "source": [
    "## ü§ù Contributing\n",
    "\n",
    "Found a bug or have a feature request? Please open an issue on GitHub!\n",
    "\n",
    "**GitHub**: https://github.com/sisird864/transformers-attention-viz\n",
    "\n",
    "### Current Limitations (v0.1.0)\n",
    "- ‚úÖ Heatmap visualization (fully working)\n",
    "- ‚ùå Flow visualization (debugging dimension issues)\n",
    "- ‚ùå Evolution visualization (debugging array indexing)\n",
    "\n",
    "### Roadmap\n",
    "- Fix flow and evolution visualizations\n",
    "- Add support for more models (Flamingo, ALIGN)\n",
    "- Real-time video attention tracking\n",
    "- 3D attention visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2744364",
   "metadata": {},
   "source": [
    "## üìö Learn More\n",
    "\n",
    "- **Documentation**: [GitHub README](https://github.com/sisird864/transformers-attention-viz)\n",
    "- **PyPI**: [transformers-attention-viz](https://pypi.org/project/transformers-attention-viz/)\n",
    "- **HuggingFace Models**: [CLIP](https://huggingface.co/openai/clip-vit-base-patch32), [BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base)\n",
    "\n",
    "Happy visualizing! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
